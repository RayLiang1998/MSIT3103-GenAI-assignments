{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888dcbdc",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 2: Transformer Architecture Exercise\n",
    "\n",
    "This notebook serves as a reference implementation for **Assignment 2** of the generative AI course.  The goal is to compare three prominent transformer architectures—**decoder‑only**, **encoder‑only**, and **encoder‑decoder**—on a common generative task.  The assignment requires training each architecture on the same dataset, evaluating their performance with common metrics, and analysing the implications of architectural differences on generative tasks and chain‑of‑thought reasoning.\n",
    "\n",
    "## Dataset selection\n",
    "\n",
    "For this exercise we use the **CNN/DailyMail** summarisation dataset (version `3.0.0`) from Hugging Face’s `datasets` library.  The dataset comprises news articles paired with human‑written summaries; each article–summary pair provides a natural input/output example for a generative model.  Because the data are already split into training/validation/test splits and are widely used for abstractive summarisation research, this dataset is appropriate for comparing generative architectures.  Although `WikiText` could be used for language modelling tasks, summarisation requires models to generate structured output given an input, which better illustrates differences between decoder‑only, encoder‑only, and encoder‑decoder designs.  For compute efficiency in this notebook we subsample the dataset (e.g. a few hundred training examples) rather than using the full corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bb4b15",
   "metadata": {},
   "source": [
    "\n",
    "## Overview of transformer architectures\n",
    "\n",
    "We train three different transformer models:\n",
    "\n",
    "* **Decoder‑only (GPT‑style):** These models consist of stacked self‑attention blocks in which each token can attend only to previous tokens (causal masking).  We use `GPT‑2` as the base model and fine‑tune it to generate a summary from an article.  Because GPT‑2 is a pure language model, we construct input prompts of the form `\"summarize: <article>\"` and train the model to predict the target summary.  During training we mask out the prompt part of the input so that the loss is computed only on the summary tokens.\n",
    "\n",
    "* **Encoder‑only (BERT‑style):** Encoder‑only models such as `BERT` learn bi‑directional contextual representations using masked language modelling (MLM).  They are not inherently generative; they excel at understanding tasks (e.g. classification, token classification).  For a fair comparison on generative tasks we fine‑tune BERT on the same corpus using MLM, combining article and summary text into a single sequence.  At evaluation time we assess perplexity and use the `fill‑mask` capability to approximate generation.  This highlights BERT’s limitations on tasks requiring free‑form generation.\n",
    "\n",
    "* **Encoder‑decoder (T5‑style):** Models like `T5` encode the input sequence with an encoder and decode the output sequence with a separate decoder.  They can perform a wide range of text‑to‑text tasks, including summarisation and question answering.  We fine‑tune `T5‑small` on the CNN/DailyMail dataset using the standard prefix `\"summarize: \"` in the input to indicate the task.  During evaluation we compute ROUGE metrics on generated summaries.\n",
    "\n",
    "The following sections implement data loading, preprocessing, model fine‑tuning, and evaluation for each architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c835e303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.10.0-cp310-none-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.25.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.10.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.9 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/rayliang/Library/Python/3.10/lib/python/site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Using cached fsspec-2026.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.10/site-packages (from torchvision) (12.1.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached markupsafe-3.0.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Downloading torch-2.10.0-cp310-none-macosx_11_0_arm64.whl (79.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.4/79.4 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.25.0-cp310-cp310-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.10.0-cp310-cp310-macosx_11_0_arm64.whl (734 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.9/734.9 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2026.1.0-py3-none-any.whl (201 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached markupsafe-3.0.3-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [torchaudio]0\u001b[0m [torchvision]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 filelock-3.20.3 fsspec-2026.1.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 sympy-1.14.0 torch-2.10.0 torchaudio-2.10.0 torchvision-0.25.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers\n",
      "  Downloading transformers-5.0.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from transformers) (3.20.3)\n",
      "Collecting huggingface-hub<2.0,>=1.3.0 (from transformers)\n",
      "  Downloading huggingface_hub-1.3.7-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rayliang/Library/Python/3.10/lib/python/site-packages (from transformers) (26.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached pyyaml-6.0.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2026.1.15-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typer-slim (from transformers)\n",
      "  Using cached typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/rayliang/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-23.0.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py310-none-any.whl.metadata (7.5 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.13.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: psutil in /Users/rayliang/Library/Python/3.10/lib/python/site-packages (from accelerate) (7.2.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from accelerate) (2.10.0)\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Downloading absl_py-2.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/rayliang/Library/Python/3.10/lib/python/site-packages (from rouge-score) (1.17.0)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.10/site-packages (from nltk) (1.5.3)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.8.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.7.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.22.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (75 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.32.2->datasets)\n",
      "  Using cached charset_normalizer-3.4.4-cp310-cp310-macosx_10_9_universal2.whl.metadata (37 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n",
      "  Using cached urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/homebrew/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/rayliang/Library/Python/3.10/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rayliang/Library/Python/3.10/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading transformers-5.0.0-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-1.3.7-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Downloading datasets-4.5.0-py3-none-any.whl (515 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading multiprocess-0.70.18-py310-none-any.whl (134 kB)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading sentencepiece-0.2.1-cp310-cp310-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached aiohttp-3.13.3-cp310-cp310-macosx_11_0_arm64.whl (491 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached multidict-6.7.1-cp310-cp310-macosx_11_0_arm64.whl (44 kB)\n",
      "Using cached yarl-1.22.0-cp310-cp310-macosx_11_0_arm64.whl (94 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached frozenlist-1.8.0-cp310-cp310-macosx_11_0_arm64.whl (49 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached propcache-0.4.1-cp310-cp310-macosx_11_0_arm64.whl (47 kB)\n",
      "Downloading pyarrow-23.0.0-cp310-cp310-macosx_12_0_arm64.whl (34.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyyaml-6.0.3-cp310-cp310-macosx_11_0_arm64.whl (174 kB)\n",
      "Using cached regex-2026.1.15-cp310-cp310-macosx_11_0_arm64.whl (288 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp310-cp310-macosx_10_9_universal2.whl (209 kB)\n",
      "Using cached urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "Using cached certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
      "Downloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "Downloading absl_py-2.4.0-py3-none-any.whl (135 kB)\n",
      "Using cached anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading pandas-2.3.3-cp310-cp310-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Downloading xxhash-3.6.0-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24986 sha256=ef16c981c1b20a6d40c4d36d1385912e66aa4ca14651d1a75c7863a53783461f\n",
      "  Stored in directory: /Users/rayliang/Library/Caches/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: pytz, xxhash, urllib3, tzdata, tqdm, shellingham, sentencepiece, safetensors, regex, pyyaml, pyarrow, propcache, multidict, idna, hf-xet, h11, fsspec, frozenlist, dill, click, charset_normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, absl-py, yarl, typer-slim, requests, pandas, nltk, multiprocess, httpcore, anyio, aiosignal, rouge-score, httpx, aiohttp, huggingface-hub, tokenizers, datasets, accelerate, transformers, evaluate\n",
      "\u001b[2K  Attempting uninstall: fsspec0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/44\u001b[0m [multidict]\n",
      "\u001b[2K    Found existing installation: fsspec 2026.1.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/44\u001b[0m [multidict]\n",
      "\u001b[2K    Uninstalling fsspec-2026.1.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/44\u001b[0m [multidict]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2026.1.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/44\u001b[0m [multidict]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44/44\u001b[0m [evaluate]/44\u001b[0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.4.0 accelerate-1.12.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 anyio-4.12.1 async-timeout-5.0.1 attrs-25.4.0 certifi-2026.1.4 charset_normalizer-3.4.4 click-8.3.1 datasets-4.5.0 dill-0.4.0 evaluate-0.4.6 frozenlist-1.8.0 fsspec-2025.10.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-1.3.7 idna-3.11 multidict-6.7.1 multiprocess-0.70.18 nltk-3.9.2 pandas-2.3.3 propcache-0.4.1 pyarrow-23.0.0 pytz-2025.2 pyyaml-6.0.3 regex-2026.1.15 requests-2.32.5 rouge-score-0.1.2 safetensors-0.7.0 sentencepiece-0.2.1 shellingham-1.5.4 tokenizers-0.22.2 tqdm-4.67.3 transformers-5.0.0 typer-slim-0.21.1 tzdata-2025.3 urllib3-2.6.3 xxhash-3.6.0 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio\n",
    "%pip install transformers datasets evaluate accelerate sentencepiece rouge-score nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da24f1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.10.0\n",
      "transformers: 5.0.0\n",
      "datasets: 4.5.0\n",
      "evaluate: 0.4.6\n",
      "cuda available: False\n"
     ]
    }
   ],
   "source": [
    "import torch, transformers, datasets, evaluate\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"datasets:\", datasets.__version__)\n",
    "print(\"evaluate:\", evaluate.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d888988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import evaluate\n",
    "from transformers import logging\n",
    "\n",
    "# Silence warnings for cleaner output\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542eddf5",
   "metadata": {},
   "source": [
    "\n",
    "## Load and inspect the dataset\n",
    "\n",
    "We load the CNN/DailyMail dataset using the Hugging Face `datasets` library.  To accelerate training for demonstration purposes we take a small subset of the training and validation sets (e.g. 500 training examples and 100 validation examples).  Each record contains two fields:\n",
    "\n",
    "* `\"article\"`: the news article text (input).\n",
    "* `\"highlights\"`: the human‑written summary (target).\n",
    "\n",
    "Below we load the dataset, inspect a few examples, and create the smaller subsets used for fine‑tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c7d3bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 287113/287113 [00:00<00:00, 366699.69 examples/s]\n",
      "Generating validation split: 100%|██████████| 13368/13368 [00:00<00:00, 337006.50 examples/s]\n",
      "Generating test split: 100%|██████████| 11490/11490 [00:00<00:00, 324861.49 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
      "Example training record: {'article': \"By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 March 2013 . | . UPDATED: . 08:07 EST, 2 March 2013 . Three members of the same family who died in a static caravan from carbon monoxide poisoning would have been unconscious 'within minutes', investigators said today. The bodies of married couple John and Audrey Cook were discovered alongside their daughter, Maureen, at the mobile home they shared on Tremarle Home Park in Camborne, west Cornwall. The inquests have now opened into the deaths last Saturday, with investigators saying the three died along with the family's pet dog, of carbon monoxide poisoning from a cooker. Tragic: The inquests have opened into the deaths of three members of the same family who were found in their static caravan last weekend. John and Audrey Cook are pictured . Awful: The family died following carbon monoxide poisoning at this caravan at the Tremarle Home Park in Camborne, Cornwall . It is also believed there was no working carbon monoxide detector in the static caravan. Cornwall Fire and Rescue Service said this would have resulted in the three being unconscious 'within minutes', . A spokesman for Cornwall coroner Dr Emma Carlyon confirmed the inquests were opened and adjourned yesterday afternoon. They will resume at a later date. Devon and Cornwall Police confirmed on Monday that carbon monoxide poisoning had been established as the cause of death. A police spokesman said the source of the poisoning was 'believed to be from incorrect operation of the gas cooker'. Poisoning: This woman left flowers outside the caravan following the deaths. It has emerged that the trio would have been unconscious 'within minutes' Touching: This tribute was left outside the caravan following news of the deaths . Early readings from experts at the site revealed a potentially lethal level of carbon monoxide present within the caravan at the time it was taken, shortly after the discovery of the bodies. Friends and neighbours have paid tribute to the trio. One . neighbour, Sonya Owen, 53, said: 'It's very distressing. I knew the . daughter, she was living her with her mum and dad. Everybody is really . upset.' Margaret Holmes, 65, who lived near the couple and their . daughter, said: 'They had lived here for around 40 years and they kept . themselves to themselves. 'I just can’t believe this has . happened, it is so sad and I am so shocked, I think we all are, you just . don’t expect this sort of thing to happen on your doorstep. 'Everyone will miss them, we used to chat a lot when we were both in the garden. 'I would just like to send my condolences to their family, I can’t imagine what they’re going through.' Nic Clark, 52, who was good friends with daughter Maureen, added: 'They were a lovely kind family, a great trio. 'Maureen . used to go out and walk her dog, a little Jack Russell, it is so sad . what has happened, I understand the dog went with them. 'They . will be sorely missed and I think everyone is just in shock at the . moment, I would like to send my condolences to the Cook family.'\", 'highlights': 'John and .\\nAudrey Cook were discovered alongside their daughter, Maureen .\\nThey were found at Tremarle Home Park in Cornwall .\\nInvestigators say the three died of carbon monoxide .\\npoisoning .', 'id': '08cf276c9eadb638e0c7fdc83ce0229c8af5d09b'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the cnn_dailymail dataset (version 3.0.0)\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# For quick experimentation, take a small subset\n",
    "train_size = 500\n",
    "val_size = 100\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "small_val_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(val_size))\n",
    "\n",
    "print(\"Dataset splits:\", dataset.keys())\n",
    "print(\"Example training record:\", small_train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b868777",
   "metadata": {},
   "source": [
    "### Dataset and Task Selection\n",
    "\n",
    "For this assignment, the CNN/DailyMail dataset (version 3.0.0) was selected to\n",
    "evaluate transformer architectures on a summarization task. The dataset consists\n",
    "of news articles paired with human-written summaries, making it well suited for\n",
    "conditional text generation.\n",
    "\n",
    "Summarization is a representative generative task that highlights architectural\n",
    "differences between decoder-only, encoder-only, and encoder-decoder models.\n",
    "Using the same dataset and task across all models allows for a fair comparison\n",
    "of training behavior, output quality, and computational efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e9c398",
   "metadata": {},
   "source": [
    "\n",
    "## Decoder‑only model: GPT‑2 fine‑tuning\n",
    "\n",
    "A decoder‑only transformer must learn to generate a summary given an input article.  We use a prompt‑based approach: the input text has the form `\"summarize: <article>\"`, and the model is trained to produce the summary tokens.  To prevent the model from learning to predict the prompt tokens, we mask the loss on the prompt portion of the sequence (by setting corresponding labels to `-100`).\n",
    "\n",
    "We use the `GPT‑2` tokenizer and model from Hugging Face.  Because GPT‑2 lacks a padding token by default, we add a pad token equal to the end‑of‑text token.  We then tokenize the inputs and construct labels accordingly.  The function below performs these steps and is mapped over the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a6c9854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 1157.25 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1279.21 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized GPT-2 input:\n",
      "summarize: By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 March 2013 . | . UPDATED: . 08:07 EST, 2 March 2013 . Three members of the same family who died in a static caravan from carbon monoxide poisoning would have been unconscious 'within minutes', investigators said today. The bodies of married couple John and Audrey Cook were discovered alongside their daughter, Maureen, at the mobile home they shared on Tremarle Home Park in Cam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model for GPT-2\n",
    "gpt2_model_name = \"gpt2\"\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Add a padding token (GPT-2 does not have one)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "# Define the preprocessing function for GPT-2\n",
    "def preprocess_gpt2(examples):\n",
    "    prefix = \"summarize: \"\n",
    "    inputs = [prefix + art for art in examples[\"article\"]]\n",
    "    targets = examples[\"highlights\"]\n",
    "\n",
    "    # Tokenize inputs and targets together, pad to max_length\n",
    "    model_inputs = gpt2_tokenizer(\n",
    "        inputs, text_target=targets,\n",
    "        max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Ensure labels are padded to max_length as well\n",
    "    if \"labels\" in model_inputs:\n",
    "        labels = model_inputs[\"labels\"]\n",
    "        for i in range(len(labels)):\n",
    "            labels[i] = labels[i] + [-100] * (512 - len(labels[i])) if len(labels[i]) < 512 else labels[i][:512]\n",
    "        model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to the small datasets\n",
    "train_gpt2 = small_train_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_gpt2 = small_val_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "print(\"Sample tokenized GPT-2 input:\")\n",
    "print(gpt2_tokenizer.decode(train_gpt2[0][\"input_ids\"][:100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0862232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs and Labels dimensions for first 10 samples:\n",
      "Sample 0: Input IDs length = 512, Labels length = 512\n",
      "Sample 1: Input IDs length = 512, Labels length = 512\n",
      "Sample 2: Input IDs length = 512, Labels length = 512\n",
      "Sample 3: Input IDs length = 512, Labels length = 512\n",
      "Sample 4: Input IDs length = 512, Labels length = 512\n",
      "Sample 5: Input IDs length = 512, Labels length = 512\n",
      "Sample 6: Input IDs length = 512, Labels length = 512\n",
      "Sample 7: Input IDs length = 512, Labels length = 512\n",
      "Sample 8: Input IDs length = 512, Labels length = 512\n",
      "Sample 9: Input IDs length = 512, Labels length = 512\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs and Labels dimensions for first 10 samples:\")\n",
    "for i in range(10):\n",
    "    print(f\"Sample {i}: Input IDs length = {len(train_gpt2[i]['input_ids'])}, Labels length = {len(train_gpt2[i]['labels'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f67ece",
   "metadata": {},
   "source": [
    "\n",
    "### GPT‑2 training configuration\n",
    "\n",
    "We use the Hugging Face `Trainer` API to fine‑tune the GPT‑2 model.  A `DataCollatorForLanguageModeling` automatically pads the inputs and labels and performs dynamic masking where appropriate (although in our custom loss masking we already set `-100` values).  The training arguments below specify a small number of epochs and batch sizes for illustration; adjust these for a full training run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7a0222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 148/148 [00:00<00:00, 2104.34it/s, Materializing param=transformer.wte.weight]             \n",
      "/opt/homebrew/lib/python3.10/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': '60', 'train_samples_per_second': '8.333', 'train_steps_per_second': '1.05', 'train_loss': '3.238', 'epoch': '1'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=3.238281492203001, metrics={'train_runtime': 59.9998, 'train_samples_per_second': 8.333, 'train_steps_per_second': 1.05, 'train_loss': 3.238281492203001, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define data collator\n",
    "data_collator_gpt2 = DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False)\n",
    "\n",
    "# Load the GPT-2 model\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Training arguments\n",
    "training_args_gpt2 = TrainingArguments(\n",
    "    output_dir=\"./gpt2-summarization\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],  # disable logging to wandb\n",
    ")\n",
    "\n",
    "# Create Trainer for GPT-2\n",
    "trainer_gpt2 = Trainer(\n",
    "    model=gpt2_model,\n",
    "    args=training_args_gpt2,\n",
    "    train_dataset=train_gpt2,\n",
    "    eval_dataset=val_gpt2,\n",
    "    data_collator=data_collator_gpt2,\n",
    ")\n",
    "\n",
    "# Uncomment the line below to train; training can take several minutes even on small subsets\n",
    "trainer_gpt2.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70f5d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary: summarize: The quick brown fox jumps over the lazy dog. The dog is a little bit shy and shy, but it is a good dog. The dog is a little shy and shy, but it is a good dog. The dog is\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a sample input\n",
    "# Original cell produced MPS/CPU device mismatch error on my machine.\n",
    "sample_input = \"summarize: The quick brown fox jumps over the lazy dog.\"\n",
    "inputs = gpt2_tokenizer(sample_input, return_tensors=\"pt\").to(device)\n",
    "outputs = gpt2_model.generate(inputs[\"input_ids\"], max_length=50, num_return_sequences=1)\n",
    "print(\"Generated summary:\", gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4efb2750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: summarize: The quick brown fox jumps over the lazy dog.\n",
      "Generated continuation (prompt removed): The dog is a little bit too lazy to get on the couch and the dog is too lazy to get on the couch. The dog is too lazy to get on the couch and\n"
     ]
    }
   ],
   "source": [
    "# Fix: CPU-only generation + postprocess (print only the generated continuation)\n",
    "\n",
    "# 1) Put model on CPU (stable on Mac without CUDA)\n",
    "gpt2_model = gpt2_model.to(\"cpu\")\n",
    "\n",
    "# 2) Prepare input on CPU as well (avoid .to(device) which may be \"mps\")\n",
    "sample_input = \"summarize: The quick brown fox jumps over the lazy dog.\"\n",
    "inputs = gpt2_tokenizer(sample_input, return_tensors=\"pt\")  # stays on CPU\n",
    "\n",
    "# 3) Generate\n",
    "outputs = gpt2_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=False,   # deterministic for easier comparison\n",
    ")\n",
    "\n",
    "# 4) Decode full text then remove the prompt part\n",
    "full_text = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "prompt_text = gpt2_tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "generated_only = full_text[len(prompt_text):].strip()\n",
    "\n",
    "print(\"Prompt:\", prompt_text)\n",
    "print(\"Generated continuation (prompt removed):\", generated_only)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a55a4c",
   "metadata": {},
   "source": [
    "### Observation (GPT-2 generation sanity check)\n",
    "After moving generation to CPU and removing the prompt from the decoded output, GPT-2 produced a continuation that is fluent but highly repetitive (e.g., repeated “too lazy to get on the couch”). This suggests the decoder-only model is sensitive to limited fine-tuning data and short training, often leading to repetition and weak summarization behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16871507",
   "metadata": {},
   "source": [
    "## What do you observe in the output?\n",
    "1. Can you postprocess the output so that it only starts printing after the input sequence?\n",
    "2. Can you iterate and improve the summary quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae6627",
   "metadata": {},
   "source": [
    "### Responses to guiding questions\n",
    "\n",
    "1. **Postprocessing the output**  \n",
    "Yes. The generated text was postprocessed by removing the original prompt tokens from the decoded output, so that only the model’s continuation is shown. This makes the generated summary easier to interpret and avoids confusion between input and output.\n",
    "\n",
    "2. **Improving summary quality**  \n",
    "The summary quality could be improved by increasing the training dataset size, training for more epochs, and applying decoding strategies such as beam search, temperature control, or repetition penalties. Using an encoder–decoder architecture (e.g., T5) is also more suitable for summarization tasks than a decoder-only model like GPT-2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29cc30",
   "metadata": {},
   "source": [
    "\n",
    "## Encoder‑only model: BERT fine‑tuning\n",
    "\n",
    "BERT uses bi‑directional self‑attention and is optimised for understanding rather than generation.  To apply BERT on our corpus we fine‑tune it using the **masked language modelling (MLM)** objective.  We concatenate the article and its summary into a single sequence and randomly mask tokens using `DataCollatorForLanguageModeling`.  While BERT cannot directly generate summaries, we compute perplexity to gauge how well it models the joint distribution of article and summary tokens.  At evaluation we also demonstrate how to use the `fill-mask` pipeline to generate single masked words as an illustration of BERT’s generative limitations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bee9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 samples of the dataset:\n",
      "small_train_dataset[0]: dict_keys(['article', 'highlights', 'id'])\n",
      "\t\tsmall_train_dataset[0]: By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 March 2013 . | . UPDATED: . 08:07 EST, 2 March 2013 . Three members of the same family who died in a sta... \n",
      "\t->\n",
      "\t\tJohn and .\n",
      "Audrey Cook were discovered alongside their daughter, Maureen .\n",
      "They were found at Tremarle Home Park in Cornwall .\n",
      "Investigators say the three died of carbon monoxide .\n",
      "poisoning .\n",
      "\t\tsmall_train_dataset[1]: UNITED NATIONS (CNN) -- A rare meeting of U.N. Security Council heads of state, led for the first time by a U.S. president, adopted a resolution focus... \n",
      "\t->\n",
      "\t\tNEW: Libya can serve as example of cooperation, White House spokesman says .\n",
      "Resolution calls for preventing nuclear weapons from being stolen, used by military .\n",
      "Obama, Russian President Dimitry Medvedev working to reduce stockpiles .\n",
      "Venezuelan president Hugo Chavez on \"Larry King Live\" tonight, 9 ET .\n",
      "\t\tsmall_train_dataset[2]: Cover-up: Former Archbishop Lord Hope allowed a paedophile priest to escape punishment for sex crimes, a judge's report claims . A former archbishop w... \n",
      "\t->\n",
      "\t\tVery Reverend Robert Waddington sexually abused choirboys for decades .\n",
      "Inquiry into the abuse has slammed Lord Hope, former Archbishop of York .\n",
      "Report claims he was made aware of misconduct 19 times but did not act .\n",
      "Despite this - he still holds influential post and sits in the House of Lords .\n",
      "\t\tsmall_train_dataset[3]: By . Kristie Lau . PUBLISHED: . 10:48 EST, 14 June 2012 . | . UPDATED: . 11:02 EST, 14 June 2012 . TLC has pulled an episode of Cake Boss from future ... \n",
      "\t->\n",
      "\t\tMonday night's episode showed Buddy Valastro tricking Anthony Bellifemine into thinking that Carmen Carerra, 27, was born as a woman .\n",
      "TLC removed the episode from future screening schedules .\n",
      "\t\tsmall_train_dataset[4]: 'The lamps are going out all over Europe. We shall not see them lit again in our lifetime' Foreign Secretary Sir Edward Grey, August 3, 1914 . Prime M... \n",
      "\t->\n",
      "\t\tPeople asked to turn out lights for hour between 10 and 11pm tomorrow .\n",
      "Gesture is in remembrance of those killed in the First World War .\n",
      "Tower Bridge and 10 Downing Street will also extinguish all but one light .\n",
      "\t\tsmall_train_dataset[5]: Roy Hodgson has come under fire for making public Raheem Sterling's admission that he was feeling tired ahead of England's Euro 2016 qualifier with Es... \n",
      "\t->\n",
      "\t\tRoy Hodgson revealed that Raheem Sterling had told him he was feeling fatigued prior to England's Euro 2016 qualifier with Estonia .\n",
      "Sterling was dropped from the starting line-up in place of Adam Lallana .\n",
      "The Liverpool man later came on as a substitute as England won 1-0 .\n",
      "Hodgson was criticised by Sky Sports pundits Jamie Redknapp and Jamie Carragher for making Sterling's admission of tiredness public .\n",
      "Wayne Rooney's free-kick preserved England's perfect start to qualifying .\n",
      "\t\tsmall_train_dataset[6]: Every frontline police officer should be offered a Taser to help fight the threat from lone-wolf terrorists, a police leader declared yesterday. Steve... \n",
      "\t->\n",
      "\t\tCalls for every police officer to be offered a Taser to fight terrorist threat .\n",
      "Police Federation set to vote on giving all frontline officers training .\n",
      "Terrorist threat level for police was raised to severe after Paris attacks .\n",
      "\t\tsmall_train_dataset[7]: By . Dan Bloom . It's one very, very small step for man - but a giant leap for antkind. Around 800 common ants, usually found pattering across pavemen... \n",
      "\t->\n",
      "\t\t800 common pavement ants are now living on International Space Station .\n",
      "Scientists will examine how they work together in low gravity to find food .\n",
      "The ants' methods can then be copied to develop 'intelligent' search robots .\n",
      "\t\tsmall_train_dataset[8]: By . Jill Reilly . PUBLISHED: . 09:26 EST, 14 June 2012 . | . UPDATED: . 13:19 EST, 14 June 2012 . A pensioner died from fatal injuries after being hi... \n",
      "\t->\n",
      "\t\tDavid Wilcockson, 71, was bowling at a ground in  Cranleigh, Surrey when the ball struck him on the head .\n",
      "Died in hospital on June 1 after 13 days in a coma .\n",
      "He was the longest-serving member of the Old Dorkinians, joining the club in 1959 .\n",
      "The pensioner had set himself a target of 3,000 wickets - and died just 101 short .\n",
      "\t\tsmall_train_dataset[9]: Workers digging an underground garage for a new hotel  recently struck something big about 30 feet below the surface. This week they uncovered it - a ... \n",
      "\t->\n",
      "\t\tThe boulder was found by a construction crew in Everett, Washington .\n",
      "The rock is bigger than an SUV and close to 19 feet long .\n"
     ]
    }
   ],
   "source": [
    "# Print first 10 samples of the dataset\n",
    "print(\"First 10 samples of the dataset:\")\n",
    "print(\"small_train_dataset[0]:\", small_train_dataset[0].keys())\n",
    "for i in range(10):\n",
    "    print(f\"\\t\\tsmall_train_dataset[{i}]: {small_train_dataset[i]['article'][:150]}... \\n\\t->\\n\\t\\t{small_train_dataset[i]['highlights']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aba715f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 3267.59 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 3052.96 examples/s]\n",
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 1999.85it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "/opt/homebrew/lib/python3.10/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': '2.261', 'grad_norm': '9.462', 'learning_rate': '1.733e-05', 'epoch': '0.8'}\n",
      "{'eval_loss': '1.804', 'eval_runtime': '2.856', 'eval_samples_per_second': '35.02', 'eval_steps_per_second': '8.754', 'epoch': '0.8'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': '48.15', 'train_samples_per_second': '10.38', 'train_steps_per_second': '2.596', 'train_loss': '2.226', 'epoch': '1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  5.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bert-mlm/tokenizer_config.json', './bert-mlm/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load tokenizer and model for BERT\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Define preprocessing: combine article and summary\n",
    "\n",
    "def preprocess_bert(examples):\n",
    "    concatenated_texts = [\n",
    "        art + \" \" + summ \n",
    "        for art, summ in zip(examples[\"article\"], examples[\"highlights\"])\n",
    "    ]\n",
    "    model_inputs = bert_tokenizer(concatenated_texts, max_length=512, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "train_bert = small_train_dataset.map(preprocess_bert, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_bert = small_val_dataset.map(preprocess_bert, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "# Data collator with MLM\n",
    "mlm_probability = 0.15\n",
    "data_collator_bert = DataCollatorForLanguageModeling(tokenizer=bert_tokenizer, mlm=True, mlm_probability=mlm_probability)\n",
    "\n",
    "# Load BERT model\n",
    "bert_model = AutoModelForMaskedLM.from_pretrained(bert_model_name)\n",
    "\n",
    "# Training arguments for BERT\n",
    "training_args_bert = TrainingArguments(\n",
    "    output_dir=\"./bert-mlm\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer_bert = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args_bert,\n",
    "    train_dataset=train_bert,\n",
    "    eval_dataset=val_bert,\n",
    "    data_collator=data_collator_bert,\n",
    ")\n",
    "\n",
    "# Uncomment the line below to train the BERT model\n",
    "trainer_bert.train()\n",
    "# save the model\n",
    "bert_model.save_pretrained(\"./bert-mlm\")\n",
    "bert_tokenizer.save_pretrained(\"./bert-mlm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49de6626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: torch.Size([1, 12, 30522]) logits\n"
     ]
    }
   ],
   "source": [
    "# Print output of bert model for sample input\n",
    "sample_input = \"The quick brown fox jumps over the lazy dog.\"\n",
    "inputs = bert_tokenizer(sample_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "bert_model = bert_model.to(inputs[\"input_ids\"].device)\n",
    "\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(**inputs)\n",
    "\n",
    "print(f\"Sample {i}: {outputs.logits.shape} logits\")\n",
    "\n",
    "# NOTE: On Apple Silicon (MPS backend), directly running BERT forward on MPS\n",
    "# may raise \"Placeholder storage has not been allocated\" errors.\n",
    "# Running this sanity check on CPU avoids the backend issue without affecting training logic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20bb0eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 2667637.37 examples/s]\n",
      "Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 2000044.57 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News dataset loaded with 1000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 10466.61 examples/s]\n",
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 1658.25it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "/opt/homebrew/lib/python3.10/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': '0.5704', 'grad_norm': '0.7775', 'learning_rate': '4.246e-05', 'epoch': '0.8'}\n",
      "{'eval_loss': '0.3276', 'eval_runtime': '28.5', 'eval_samples_per_second': '35.09', 'eval_steps_per_second': '4.386', 'epoch': '0.8'}\n",
      "{'loss': '0.302', 'grad_norm': '0.4653', 'learning_rate': '2.708e-05', 'epoch': '1.6'}\n",
      "{'eval_loss': '0.2295', 'eval_runtime': '28.9', 'eval_samples_per_second': '34.61', 'eval_steps_per_second': '4.326', 'epoch': '1.6'}\n",
      "{'loss': '0.2513', 'grad_norm': '0.3994', 'learning_rate': '1.169e-05', 'epoch': '2.4'}\n",
      "{'eval_loss': '0.1863', 'eval_runtime': '28.8', 'eval_samples_per_second': '34.72', 'eval_steps_per_second': '4.34', 'epoch': '2.4'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': '216.6', 'train_samples_per_second': '13.85', 'train_steps_per_second': '1.732', 'train_loss': '0.3445', 'epoch': '3'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.3445113271077474, metrics={'train_runtime': 216.5681, 'train_samples_per_second': 13.852, 'train_steps_per_second': 1.732, 'train_loss': 0.3445113271077474, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze the BERT model and train a classifier on top\n",
    "# load a news sentiment classification dataset\n",
    "from datasets import load_dataset\n",
    "news_dataset = load_dataset(\"ag_news\", split=\"train[:1000]\")\n",
    "print(\"News dataset loaded with\", len(news_dataset), \"samples.\")\n",
    "# Preprocess the dataset for BERT\n",
    "def preprocess_news(examples):\n",
    "    model_inputs = bert_tokenizer(examples[\"text\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = examples[\"label\"]\n",
    "    return model_inputs\n",
    "train_news = news_dataset.map(preprocess_news, batched=True, remove_columns=news_dataset.column_names)\n",
    "# Use the trained model as a feature extractor and add a classification head\n",
    "def get_news_classifier_model():\n",
    "    \"\"\"\n",
    "    load the trained BERT model and add a classification head\n",
    "    \"\"\"\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"./bert-mlm\") # Load the trained BERT model\n",
    "    # Freeze the BERT model\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Add a classification head\n",
    "    model.classifier = torch.nn.Linear(model.config.hidden_size, 4)  # 4 classes for AG News\n",
    "    return model\n",
    "\n",
    "news_classifier_model = get_news_classifier_model()\n",
    "# Training arguments for news classifier\n",
    "news_training_args = TrainingArguments(\n",
    "    output_dir=\"./news-classifier\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    ")\n",
    "# Create Trainer for news classifier\n",
    "news_trainer = Trainer(\n",
    "    model=news_classifier_model,\n",
    "    args=news_training_args,\n",
    "    train_dataset=train_news,\n",
    "    eval_dataset=train_news,  # For simplicity, using the same dataset for eval\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=bert_tokenizer, mlm=False),\n",
    ")\n",
    "# Uncomment the line below to train the news classifier\n",
    "news_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3e0b860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1840.46it/s, Materializing param=bert.pooler.dense.weight]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News dataset loaded with 1000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': '1.121', 'grad_norm': '8.86', 'learning_rate': '4.831e-05', 'epoch': '0.2'}\n",
      "{'eval_loss': '0.6168', 'eval_runtime': '24.75', 'eval_samples_per_second': '40.41', 'eval_steps_per_second': '20.21', 'epoch': '0.2'}\n",
      "{'loss': '0.7066', 'grad_norm': '0.08315', 'learning_rate': '4.486e-05', 'epoch': '0.4'}\n",
      "{'eval_loss': '0.5769', 'eval_runtime': '24.83', 'eval_samples_per_second': '40.28', 'eval_steps_per_second': '20.14', 'epoch': '0.4'}\n",
      "{'loss': '0.741', 'grad_norm': '0.7544', 'learning_rate': '4.141e-05', 'epoch': '0.6'}\n",
      "{'eval_loss': '0.5107', 'eval_runtime': '24.96', 'eval_samples_per_second': '40.06', 'eval_steps_per_second': '20.03', 'epoch': '0.6'}\n",
      "{'loss': '0.5309', 'grad_norm': '0.04578', 'learning_rate': '3.797e-05', 'epoch': '0.8'}\n",
      "{'eval_loss': '0.7697', 'eval_runtime': '24.8', 'eval_samples_per_second': '40.31', 'eval_steps_per_second': '20.16', 'epoch': '0.8'}\n",
      "{'loss': '0.6845', 'grad_norm': '0.07681', 'learning_rate': '3.452e-05', 'epoch': '1'}\n",
      "{'eval_loss': '0.4145', 'eval_runtime': '24.81', 'eval_samples_per_second': '40.31', 'eval_steps_per_second': '20.15', 'epoch': '1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\n",
      "/opt/homebrew/lib/python3.10/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': '0.4135', 'grad_norm': '3.235', 'learning_rate': '3.107e-05', 'epoch': '1.2'}\n",
      "{'eval_loss': '0.474', 'eval_runtime': '25.19', 'eval_samples_per_second': '39.7', 'eval_steps_per_second': '19.85', 'epoch': '1.2'}\n",
      "{'loss': '0.5003', 'grad_norm': '15.19', 'learning_rate': '2.762e-05', 'epoch': '1.4'}\n",
      "{'eval_loss': '0.3859', 'eval_runtime': '26.79', 'eval_samples_per_second': '37.32', 'eval_steps_per_second': '18.66', 'epoch': '1.4'}\n",
      "{'loss': '0.4847', 'grad_norm': '3.289', 'learning_rate': '2.417e-05', 'epoch': '1.6'}\n",
      "{'eval_loss': '0.2493', 'eval_runtime': '29.98', 'eval_samples_per_second': '33.35', 'eval_steps_per_second': '16.68', 'epoch': '1.6'}\n",
      "{'loss': '0.415', 'grad_norm': '0.06056', 'learning_rate': '2.072e-05', 'epoch': '1.8'}\n",
      "{'eval_loss': '0.236', 'eval_runtime': '31.72', 'eval_samples_per_second': '31.53', 'eval_steps_per_second': '15.76', 'epoch': '1.8'}\n",
      "{'loss': '0.4038', 'grad_norm': '0.2159', 'learning_rate': '1.728e-05', 'epoch': '2'}\n",
      "{'eval_loss': '0.155', 'eval_runtime': '29.71', 'eval_samples_per_second': '33.65', 'eval_steps_per_second': '16.83', 'epoch': '2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  4.24it/s]\n",
      "/opt/homebrew/lib/python3.10/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': '0.09658', 'grad_norm': '0.01684', 'learning_rate': '1.383e-05', 'epoch': '2.2'}\n",
      "{'eval_loss': '0.1539', 'eval_runtime': '28.53', 'eval_samples_per_second': '35.05', 'eval_steps_per_second': '17.53', 'epoch': '2.2'}\n",
      "{'loss': '0.3232', 'grad_norm': '18.48', 'learning_rate': '1.038e-05', 'epoch': '2.4'}\n",
      "{'eval_loss': '0.136', 'eval_runtime': '27.6', 'eval_samples_per_second': '36.24', 'eval_steps_per_second': '18.12', 'epoch': '2.4'}\n",
      "{'loss': '0.1639', 'grad_norm': '0.03202', 'learning_rate': '6.931e-06', 'epoch': '2.6'}\n",
      "{'eval_loss': '0.1117', 'eval_runtime': '27.05', 'eval_samples_per_second': '36.97', 'eval_steps_per_second': '18.49', 'epoch': '2.6'}\n",
      "{'loss': '0.1492', 'grad_norm': '108.3', 'learning_rate': '3.483e-06', 'epoch': '2.8'}\n",
      "{'eval_loss': '0.1039', 'eval_runtime': '26.82', 'eval_samples_per_second': '37.28', 'eval_steps_per_second': '18.64', 'epoch': '2.8'}\n",
      "{'loss': '0.1343', 'grad_norm': '0.05669', 'learning_rate': '3.448e-08', 'epoch': '3'}\n",
      "{'eval_loss': '0.08938', 'eval_runtime': '26.62', 'eval_samples_per_second': '37.56', 'eval_steps_per_second': '18.78', 'epoch': '3'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': '671.7', 'train_samples_per_second': '4.466', 'train_steps_per_second': '2.233', 'train_loss': '0.4579', 'epoch': '3'}\n",
      "Sample news input: Breaking news: The stock market crashes as investors panic.\n",
      "Predicted class: 2\n",
      "Available classes: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "# Print sample output of the news classifier\n",
    "# The previous approach used AutoModelForMaskedLM, which is not suitable for classification.\n",
    "# Instead, use AutoModelForSequenceClassification for news classification.\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import evaluate\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a BERT model with a classification head\n",
    "news_classifier_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    bert_model_name, num_labels=4\n",
    ").to(device)\n",
    "\n",
    "# Training arguments for news classifier remain the same\n",
    "news_training_args = TrainingArguments(\n",
    "    output_dir=\"./news-classifier\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "# Load the AG News dataset\n",
    "from datasets import load_dataset\n",
    "news_dataset = load_dataset(\"ag_news\", split=\"train[:1000]\")\n",
    "print(\"News dataset loaded with\", len(news_dataset), \"samples.\")\n",
    "\n",
    "# Preprocess the dataset for BERT\n",
    "def preprocess_news(examples):\n",
    "    model_inputs = bert_tokenizer(examples[\"text\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = examples[\"label\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_news = news_dataset.map(preprocess_news, batched=True, remove_columns=news_dataset.column_names)\n",
    "\n",
    "# Create Trainer for news classifier\n",
    "news_trainer = Trainer(\n",
    "    model=news_classifier_model,\n",
    "    args=news_training_args,\n",
    "    train_dataset=train_news,\n",
    "    eval_dataset=train_news,  # For simplicity, using the same dataset for eval\n",
    "    # tokenizer=bert_tokenizer,\n",
    ")\n",
    "# Uncomment the line below to train the news classifier\n",
    "news_trainer.train()\n",
    "\n",
    "# Print sample output of the news classifier\n",
    "sample_news_input = \"Breaking news: The stock market crashes as investors panic.\"\n",
    "# NOTE:\n",
    "# MPS (Apple Silicon) can throw \"Placeholder storage has not been allocated\" for some forward paths.\n",
    "# For this quick inference demo, we run the classifier on CPU for stability.\n",
    "\n",
    "news_classifier_model_cpu = news_classifier_model.to(\"cpu\")\n",
    "inputs_cpu = bert_tokenizer(sample_news_input, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = news_classifier_model_cpu(**inputs_cpu)\n",
    "\n",
    "pred_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "print(f\"Sample news input: {sample_news_input}\")\n",
    "print(f\"Predicted class: {pred_class}\")\n",
    "print(\"Available classes:\", news_dataset.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261a65ea",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "BERT is used here primarily for representation learning rather than text generation, leveraging masked language modeling and a classification head for downstream tasks.\n",
    "\n",
    "The classification demo is reasonable, as the sample market-related news is correctly mapped to the **Business** category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e05025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install seaborn into the *current Jupyter kernel* environment.\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install -q seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c0127fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[207   1   3   1]\n",
      " [  0 142   0   0]\n",
      " [  0   0 161  13]\n",
      " [  1   1   2 468]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "# Get predictions for the entire training set\n",
    "predictions = []\n",
    "for i in range(0, len(train_news), 32):  # Batch size of 32\n",
    "    batch = train_news[i:i+32]\n",
    "    # batch is a dict with keys: input_ids, token_type_ids, attention_mask, labels\n",
    "    # You need to decode input_ids to get the original text if you want to print or use text\n",
    "    # For inference, use input_ids, attention_mask, etc.\n",
    "    inputs = {\n",
    "        \"input_ids\": torch.tensor(batch[\"input_ids\"]).to(device),\n",
    "        \"attention_mask\": torch.tensor(batch[\"attention_mask\"]).to(device)\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        outputs = news_classifier_model(**inputs)\n",
    "    preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "    predictions.extend(preds)\n",
    "# Get true labels\n",
    "true_labels = np.array(batch[\"labels\"] for batch in train_news)  # This line should be fixed\n",
    "# Instead, collect all labels from train_news:\n",
    "true_labels = np.array(train_news[\"labels\"])\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "# Plot confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=news_dataset.features[\"label\"].names,\n",
    "            yticklabels=news_dataset.features[\"label\"].names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix for News Classifier\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1087a8",
   "metadata": {},
   "source": [
    "### Confusion Matrix Interpretation\n",
    "\n",
    "The confusion matrix shows that the classifier performs strongly overall, with most predictions concentrated along the diagonal, indicating correct classifications. The *Sci/Tech* and *World* categories achieve particularly high accuracy, while a small number of misclassifications occur between *Business* and *Sci/Tech*, suggesting some semantic overlap between these categories. Overall, the results indicate that the fine-tuned BERT model is effective for news topic classification on the AG News dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ee761",
   "metadata": {},
   "source": [
    "\n",
    "## Encoder‑decoder model: T5 fine‑tuning\n",
    "\n",
    "T5 is a text‑to‑text model that uses a separate encoder and decoder.  It naturally handles generative tasks such as summarisation.  We prepend the prefix `\"summarize: \"` to each article, then tokenize the input and the summary separately.  A `DataCollatorForSeq2Seq` takes care of padding the inputs and shifting the decoder labels.  During evaluation we use greedy decoding to produce summaries and compute ROUGE scores against the reference summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b14ed723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
      "Example training record: {'article': \"By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 March 2013 . | . UPDATED: . 08:07 EST, 2 March 2013 . Three members of the same family who died in a static caravan from carbon monoxide poisoning would have been unconscious 'within minutes', investigators said today. The bodies of married couple John and Audrey Cook were discovered alongside their daughter, Maureen, at the mobile home they shared on Tremarle Home Park in Camborne, west Cornwall. The inquests have now opened into the deaths last Saturday, with investigators saying the three died along with the family's pet dog, of carbon monoxide poisoning from a cooker. Tragic: The inquests have opened into the deaths of three members of the same family who were found in their static caravan last weekend. John and Audrey Cook are pictured . Awful: The family died following carbon monoxide poisoning at this caravan at the Tremarle Home Park in Camborne, Cornwall . It is also believed there was no working carbon monoxide detector in the static caravan. Cornwall Fire and Rescue Service said this would have resulted in the three being unconscious 'within minutes', . A spokesman for Cornwall coroner Dr Emma Carlyon confirmed the inquests were opened and adjourned yesterday afternoon. They will resume at a later date. Devon and Cornwall Police confirmed on Monday that carbon monoxide poisoning had been established as the cause of death. A police spokesman said the source of the poisoning was 'believed to be from incorrect operation of the gas cooker'. Poisoning: This woman left flowers outside the caravan following the deaths. It has emerged that the trio would have been unconscious 'within minutes' Touching: This tribute was left outside the caravan following news of the deaths . Early readings from experts at the site revealed a potentially lethal level of carbon monoxide present within the caravan at the time it was taken, shortly after the discovery of the bodies. Friends and neighbours have paid tribute to the trio. One . neighbour, Sonya Owen, 53, said: 'It's very distressing. I knew the . daughter, she was living her with her mum and dad. Everybody is really . upset.' Margaret Holmes, 65, who lived near the couple and their . daughter, said: 'They had lived here for around 40 years and they kept . themselves to themselves. 'I just can’t believe this has . happened, it is so sad and I am so shocked, I think we all are, you just . don’t expect this sort of thing to happen on your doorstep. 'Everyone will miss them, we used to chat a lot when we were both in the garden. 'I would just like to send my condolences to their family, I can’t imagine what they’re going through.' Nic Clark, 52, who was good friends with daughter Maureen, added: 'They were a lovely kind family, a great trio. 'Maureen . used to go out and walk her dog, a little Jack Russell, it is so sad . what has happened, I understand the dog went with them. 'They . will be sorely missed and I think everyone is just in shock at the . moment, I would like to send my condolences to the Cook family.'\", 'highlights': 'John and .\\nAudrey Cook were discovered alongside their daughter, Maureen .\\nThey were found at Tremarle Home Park in Cornwall .\\nInvestigators say the three died of carbon monoxide .\\npoisoning .', 'id': '08cf276c9eadb638e0c7fdc83ce0229c8af5d09b'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1015.04 examples/s]\n",
      "\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "C:\\Users\\onurb\\AppData\\Local\\Temp\\ipykernel_6872\\3507327657.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_t5 = Trainer(\n",
      "C:\\Users\\onurb\\AppData\\Local\\Temp\\ipykernel_6872\\3507327657.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_t5 = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=2.4141407315693204, metrics={'train_runtime': 37.7167, 'train_samples_per_second': 13.257, 'train_steps_per_second': 1.67, 'total_flos': 67661913194496.0, 'train_loss': 2.4141407315693204, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "# Load the cnn_dailymail dataset (version 3.0.0)\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# For quick experimentation, take a small subset\n",
    "train_size = 500\n",
    "val_size = 100\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "small_val_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(val_size))\n",
    "print(\"Dataset splits:\", dataset.keys())\n",
    "print(\"Example training record:\", small_train_dataset[0])\n",
    "\n",
    "# Load T5 tokenizer and model\n",
    "t5_model_name = \"t5-small\"\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(t5_model_name)\n",
    "\n",
    "def preprocess_t5(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = t5_tokenizer(inputs, max_length=512, truncation=True)\n",
    "\n",
    "    # Tokenize targets\n",
    "    labels = t5_tokenizer(examples[\"highlights\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_t5 = small_train_dataset.map(preprocess_t5, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_t5 = small_val_dataset.map(preprocess_t5, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "# Data collator for seq2seq tasks\n",
    "data_collator_t5 = DataCollatorForSeq2Seq(tokenizer=t5_tokenizer, model=t5_model_name)\n",
    "\n",
    "# Load T5 model\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_model_name)\n",
    "\n",
    "# Training arguments for T5\n",
    "training_args_t5 = TrainingArguments(\n",
    "    output_dir=\"./t5-summarization\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer_t5 = Trainer(\n",
    "    model=t5_model,\n",
    "    args=training_args_t5,\n",
    "    train_dataset=train_t5,\n",
    "    eval_dataset=val_t5,\n",
    "    data_collator=data_collator_t5,\n",
    "    tokenizer=t5_tokenizer,\n",
    ")\n",
    "\n",
    "# Uncomment the line below to train the T5 model\n",
    "trainer_t5.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb1ffa7",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation\n",
    "\n",
    "After fine‑tuning the models (training steps are commented out by default), we evaluate them on the validation subset. Different metrics are appropriate for each architecture:\n",
    "\n",
    "* **GPT‑2** (decoder‑only): We generate summaries using greedy decoding and compute ROUGE metrics (ROUGE‑1, ROUGE‑2, ROUGE‑L). We also compute perplexity using the loss returned by the trainer.\n",
    "\n",
    "* **BERT** (encoder‑only): BERT is not designed to generate full sequences; instead we use it for downstream tasks such as text classification. For a classification scenario, the evaluation metrics are typically confusion matrix and F1-score.\n",
    "\n",
    "* **T5** (encoder‑decoder): We generate summaries using greedy decoding and compute ROUGE metrics.  Perplexity is computed similarly to GPT‑2 by exponentiating the validation loss.\n",
    "\n",
    "The code below demonstrates evaluation routines for each model. Running these functions requires trained models; if you skipped training above, the evaluation will use the pre‑trained weights and therefore will not yield good summarization quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4253cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\MSIT3103-GenAI-workspace\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Evaluation Report: {\n",
      "  \"0\": {\n",
      "    \"precision\": 1.0,\n",
      "    \"recall\": 0.9523809523809523,\n",
      "    \"f1-score\": 0.975609756097561,\n",
      "    \"support\": 21.0\n",
      "  },\n",
      "  \"1\": {\n",
      "    \"precision\": 1.0,\n",
      "    \"recall\": 1.0,\n",
      "    \"f1-score\": 1.0,\n",
      "    \"support\": 16.0\n",
      "  },\n",
      "  \"2\": {\n",
      "    \"precision\": 1.0,\n",
      "    \"recall\": 1.0,\n",
      "    \"f1-score\": 1.0,\n",
      "    \"support\": 18.0\n",
      "  },\n",
      "  \"3\": {\n",
      "    \"precision\": 0.9782608695652174,\n",
      "    \"recall\": 1.0,\n",
      "    \"f1-score\": 0.989010989010989,\n",
      "    \"support\": 45.0\n",
      "  },\n",
      "  \"accuracy\": 0.99,\n",
      "  \"macro avg\": {\n",
      "    \"precision\": 0.9945652173913043,\n",
      "    \"recall\": 0.9880952380952381,\n",
      "    \"f1-score\": 0.9911551862771375,\n",
      "    \"support\": 100.0\n",
      "  },\n",
      "  \"weighted avg\": {\n",
      "    \"precision\": 0.9902173913043478,\n",
      "    \"recall\": 0.99,\n",
      "    \"f1-score\": 0.9899329938354329,\n",
      "    \"support\": 100.0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define ROUGE metric\n",
    "evaluate_rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics_rouge(preds, refs):\n",
    "    # Compute ROUGE scores; use newline separation between sentences in each text\n",
    "    result = evaluate_rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    return {k: round(v * 100, 2) for k, v in result.items()}\n",
    "\n",
    "# Function to generate summaries with GPT-2\n",
    "def evaluate_gpt2(model, tokenizer, dataset, num_samples=10):\n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "    for i, example in enumerate(dataset.select(range(num_samples))):\n",
    "        prompt = \"summarize: \" + example[\"article\"]\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_length=128)\n",
    "        summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        preds.append(summary)\n",
    "        refs.append(example[\"highlights\"])\n",
    "    rouge_scores = compute_metrics_rouge(preds, refs)\n",
    "    return rouge_scores\n",
    "\n",
    "# Function to compute perplexity from evaluation loss\n",
    "def compute_perplexity(eval_output):\n",
    "    loss = eval_output[\"eval_loss\"]\n",
    "    return round(torch.exp(torch.tensor(loss)).item(), 3)\n",
    "\n",
    "# Evaluate GPT-2 (if trained) -- example usage\n",
    "gpt2_eval_results = trainer_gpt2.evaluate()\n",
    "gpt2_perplexity = compute_perplexity(gpt2_eval_results)\n",
    "rouge_gpt2 = evaluate_gpt2(gpt2_model, gpt2_tokenizer, small_val_dataset)\n",
    "print(\"GPT-2 Perplexity:\", gpt2_perplexity)\n",
    "print(\"GPT-2 ROUGE:\", rouge_gpt2)\n",
    "\n",
    "# BERT evaluation: compute F1 score - sklearn classification report\n",
    "from sklearn.metrics import classification_report\n",
    "def evaluate_bert(model, dataset):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for i in range(0, len(dataset), 32):  # Batch size of 32\n",
    "        batch = dataset[i:i+32]\n",
    "        # batch is a dict with keys: input_ids, token_type_ids, attention_mask, labels\n",
    "        # You need to decode input_ids to get the original text if you want to print or use text\n",
    "        # For inference, use input_ids, attention_mask, etc.\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.tensor(batch[\"input_ids\"]).to(device),\n",
    "            \"attention_mask\": torch.tensor(batch[\"attention_mask\"]).to(device)\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = news_classifier_model(**inputs)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "    true_labels = np.array(dataset[\"labels\"])\n",
    "    # Generate classification report\n",
    "    report = classification_report(true_labels, predictions, output_dict=True)\n",
    "    return report\n",
    "# Evaluate on random 100 samples\n",
    "bert_eval_results = evaluate_bert(news_classifier_model, train_news.shuffle(seed=42).select(range(100)))\n",
    "import json\n",
    "print(\"BERT Evaluation Report:\", json.dumps(bert_eval_results, indent=2))\n",
    "\n",
    "# Demonstrate fill-mask prediction with BERT\n",
    "# from transformers import pipeline\n",
    "# fill_mask = pipeline(\"fill-mask\", model=bert_model, tokenizer=bert_tokenizer)\n",
    "# sentence = \"The weather today is [MASK].\"\n",
    "# print(fill_mask(sentence))\n",
    "\n",
    "# T5 evaluation\n",
    "# t5_eval_results = trainer_t5.evaluate()\n",
    "# t5_perplexity = compute_perplexity(t5_eval_results)\n",
    "\n",
    "# def evaluate_t5(model, tokenizer, dataset, num_samples=10):\n",
    "#     model.eval()\n",
    "#     preds, refs = [], []\n",
    "#     for i, example in enumerate(dataset.select(range(num_samples))):\n",
    "#         input_text = \"summarize: \" + example[\"article\"]\n",
    "#         inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "#         with torch.no_grad():\n",
    "#             output_ids = model.generate(**inputs, max_length=128)\n",
    "#         summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "#         preds.append(summary)\n",
    "#         refs.append(example[\"highlights\"])\n",
    "#     rouge_scores = compute_metrics_rouge(preds, refs)\n",
    "#     return rouge_scores\n",
    "\n",
    "# rouge_t5 = evaluate_t5(t5_model, t5_tokenizer, small_val_dataset)\n",
    "# print(\"T5 Perplexity:\", t5_perplexity)\n",
    "# print(\"T5 ROUGE:\", rouge_t5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8a2bb",
   "metadata": {},
   "source": [
    "\n",
    "## Analysis and discussion\n",
    "\n",
    "After fine‑tuning the models and running the evaluation routines, you should fill in a comparison of the results.  Typical observations include:\n",
    "\n",
    "- **Decoder‑only (GPT‑2):** GPT‑2 fine‑tuned on a summarization corpus learns to generate coherent summaries.  Its perplexity should decrease significantly compared with the pre‑trained model, and ROUGE scores should improve.  Because GPT‑2 has no separate encoder, it must memorize how to map the input prompt to the desired output, which can make training less sample‑efficient for conditional tasks.  However, at inference time GPT‑2 generates outputs quickly via a single decoder.\n",
    "\n",
    "- **Encoder‑only (BERT):** BERT excels at understanding tasks but struggles with generative tasks.  MLM fine‑tuning improves its perplexity on the article‑summary text, but it cannot generate full summaries.  The `fill‑mask` pipeline can fill individual tokens, but the lack of an auto‑regressive decoder makes long‑form generation impractical.  This illustrates why encoder‑only architectures are not suited for free‑form text generation.\n",
    "\n",
    "- **Encoder‑decoder (T5):** T5 is designed for text‑to‑text tasks and typically achieves the best summarization scores among the three models when fine‑tuned properly.  Its separate encoder compresses the input, and the decoder generates output conditioned on the encoded context.  T5 often yields higher ROUGE scores and lower perplexity than GPT‑2 on summarization because the architecture explicitly models conditional generation.  The trade‑off is increased computational cost due to the encoder and decoder.\n",
    "\n",
    "### Chain‑of‑thought (CoT) reasoning\n",
    "\n",
    "Chain‑of‑thought reasoning refers to models generating intermediate reasoning steps before arriving at a final answer.  Decoder‑only models (like GPT‑2 and GPT‑3) naturally support CoT prompting because they generate text token by token.  Encoder‑decoder models like T5 can also perform CoT when prompted appropriately (e.g. instructing the model to \"think step by step\").  Encoder‑only models lack a decoding mechanism and therefore are not directly applicable to CoT generation.  In practice, CoT reasoning quality improves with larger models and more sophisticated training (e.g. instruction‑tuning or reinforcement learning with human feedback), which are beyond the scope of this introductory exercise.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook we implemented and compared three transformer architectures on a common summarization task using the CNN/DailyMail dataset.  We demonstrated how to fine‑tune a decoder‑only model (GPT‑2), an encoder‑only model (BERT), and an encoder‑decoder model (T5).  The code illustrated data preprocessing, training setups, and evaluation routines using ROUGE and perplexity metrics.  While only small subsets of the dataset were used for demonstration purposes, you should expand the training data and adjust hyperparameters for a thorough experiment.  The analysis underscores the strengths and limitations of each architecture and highlights why encoder‑decoder models are generally preferred for conditional text generation tasks like summarization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81364b3c",
   "metadata": {},
   "source": [
    "# Assignment 2: Transformer Architecture Exercise\n",
    "Use this notebook as a starting point and expand on your understanding of transformer models by completing the following structured tasks. You are encouraged to experiment, analyze, and critically reflect on your findings in your report.\n",
    "\n",
    "## Part 1: Model Training & Implementation\n",
    "### 1. Dataset Preparation\n",
    "- Choose one standard text dataset suitable for generative tasks. Options include:\n",
    "  - CNN/DailyMail → summarization\n",
    "  - WikiText-2 → language modeling (text generation)\n",
    "  - SQuAD v1.1 → question answering\n",
    "- Briefly describe why you selected this dataset and what task you’ll evaluate (summarization, QA, or text generation).\n",
    "- Show how you preprocessed the data (tokenization, train/val split, max length, etc.).\n",
    "\n",
    "### 2. Model Implementation\n",
    "\n",
    "Implement and train the following:\n",
    "- Decoder-only model (GPT-style): e.g., GPT-2 small from Hugging Face.\n",
    "- Encoder-only model (BERT-style): e.g., BERT-base, used for masked-language-modeling or extractive QA/summarization.\n",
    "- Encoder-decoder model (T5-style): e.g., T5-small, trained for the same dataset/task as the other two.\n",
    "\n",
    "### 3. Training Documentation\n",
    "\n",
    "- Document your training setup (batch size, learning rate, optimizer, epochs, hardware).\n",
    "- Save a few training/validation loss curves or logs to show how training progressed.\n",
    "- Mention any difficulties you faced and how you addressed them (e.g., memory limits, convergence).\n",
    "\n",
    "## Part 2: Evaluation & Analysis\n",
    "\n",
    "### 4. Performance Evaluation\n",
    "\n",
    "- Evaluate all three models on the same task.\n",
    "- Report results using at least two metrics:\n",
    "  - Text generation/summarization: BLEU, ROUGE, perplexity\n",
    "  - Question answering: F1, Exact Match (EM), BLEU\n",
    "- Include 1–2 sample outputs per model to illustrate qualitative differences.\n",
    "\n",
    "### 5. Comparative Discussion\n",
    "\n",
    "- Compare the strengths and weaknesses of each architecture on your chosen task.\n",
    "- Suggested angles:\n",
    "\n",
    "  - Decoder-only: fluent text generation, but weaker at bidirectional context.\n",
    "  - Encoder-only: strong understanding of context, but not designed for open generation.\n",
    "  - Encoder-decoder: flexible, strong on conditional generation tasks (summarization, QA).\n",
    "\n",
    "- Which model seemed easiest to fine-tune?\n",
    "- Which produced the best outputs on your dataset?\n",
    "- Which was the most efficient (speed, memory)?\n",
    "\n",
    "### 6. Reflections on Applicability\n",
    "\n",
    "- In what real-world scenarios would you prefer each architecture?\n",
    "- Briefly note whether you think CoT reasoning would have helped these models if you had added it (conceptual discussion only—no experiments required)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
